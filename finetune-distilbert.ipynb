{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hugging Face and Sagemaker: fine-tuning DistilBERT with Amazon Polarity dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In this demo, you will use the Hugging Faces `transformers` and `datasets` library with Amazon SageMaker to fine-tune a pre-trained transformer on binary text classification. In particular, you will use the pre-trained DistilBERT model with the Amazon Reviews Polarity dataset.\n",
    "You will then deploy the resulting model for inference using SageMaker Endpoint.\n",
    "\n",
    "### The model\n",
    "\n",
    "You'll be using an offshoot of [BERT](https://arxiv.org/abs/1810.04805) called [DistilBERT](https://arxiv.org/abs/1910.01108) that is smaller, and so faster and cheaper for both training and inference. A pre-trained model is available in the [`transformers`](https://huggingface.co/transformers/) library from [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "### The data\n",
    "\n",
    "The [Amazon Reviews Polarity dataset](https://github.com/dsk78/Text-Classification---Amazon-Reviews-Polarity) consists of reviews from Amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. It's avalaible under the [`amazon_polarity`](https://huggingface.co/datasets/amazon_polarity) dataset on [Hugging Face](https://huggingface.co/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook was tested in Amazon SageMaker Studio on a **ml.m5.large** instance with **Python 3 (Data Science)** kernel._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies\n",
    "First, you need to install the dependecies required."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -qq \"sagemaker>=2.48.0\" --upgrade\n",
    "!pip install -qq torch==1.7.1 --upgrade\n",
    "!pip install -qq sagemaker-huggingface-inference-toolkit \n",
    "!pip install -qq transformers==4.6.1 \"datasets[s3]\"\n",
    "!pip install -qq ipywidgets\n",
    "!pip install -qq watermark \n",
    "!pip install -qq \"seaborn>=0.11.0\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are running this on a SageMaker environment, make sure to reboot the Kernel via the dropdown menu at the top after you've installed the above dependencies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Development environment "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import ProfilerConfig, DebuggerHookConfig, Rule, ProfilerRule, rule_configs\n",
    "import sagemaker.huggingface\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from textwrap import wrap\n",
    "\n",
    "import boto3\n",
    "import pprint\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize'] = 17, 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up SageMaker session and bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "\n",
    "The data preparation is straightforward as you're using the `datasets` library to download and preprocess the `\n",
    "amazon_polarity` dataset directly from Hugging face. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_name = 'amazon_polarity'\n",
    "\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "train_dataset = train_dataset.shuffle().select(range(5000)) # limiting the dataset size to speed up the training during the demo\n",
    "test_dataset = test_dataset.shuffle().select(range(1000))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_dataset.column_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at en example from the training dataset. This allows us to understand what is the format that is expected by Hugging Face Transformers library for input."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is already well balanced"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.countplot(x=train_dataset['label'])\n",
    "plt.xlabel('label');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the dataset to be used with PyTorch\n",
    "You now need to convert the dataset for training. This means using a tokenizer and getting the PyTorch tensors. Hugging Face provides an [`AutoTokenizer`](https://huggingface.co/transformers/model_doc/auto.html#autotokenizer)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This downloads the tokenizer:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer_name = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And this tokenize our training and testing datasets and then set them to the PyTorch format:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Helper function to get the content to tokenize\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['content'], padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "# Set the format to PyTorch\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uploading the data to S3\n",
    "\n",
    "Now that the data as been processed you can upload it to S3 for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# Upload to S3\n",
    "s3 = S3FileSystem()\n",
    "s3_prefix = f'samples/datasets/{dataset_name}'\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)\n",
    "\n",
    "print(f'Uploaded training data to {training_input_path}')\n",
    "print(f'Uploaded testing data to {test_input_path}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job you need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator you define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in.\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)\n",
    "```\n",
    "When you create a SageMaker training job, SageMaker takes care of starting and managing all the required compute instances with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container local storage at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 5 --model_name distilbert-base-cased --token_name distilbert-base-cased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. The training script expect the `HuggingFace` model and token name so it can retrieve them.\n",
    "\n",
    "Sagemaker is providing other useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can inspect the training script by running the next cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ./scripts/train.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating an Estimator and start a training job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Name your training job so you can follow it:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = 'distilbert-base-cased'\n",
    "import datetime\n",
    "ct = datetime.datetime.now() \n",
    "current_time = str(ct.now()).replace(\":\", \"-\").replace(\" \", \"-\")[:19]\n",
    "training_job_name=f'finetune-{model_name}-{current_time}'\n",
    "print( training_job_name )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hyperparameters={'epochs': 3,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name': model_name,\n",
    "                 'tokenizer_name': tokenizer_name,\n",
    "                 'output_dir':'/opt/ml/checkpoints',\n",
    "                 }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision', 'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_recall', 'Regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6', \n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            max_run=36000, # expected max run in seconds\n",
    "                        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Starts the training job using the estimator fit function:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path}, wait=False, job_name=training_job_name )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wait for the training to finish. Training takes approximately 10 mins to complete."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sess.wait_for_job(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training metrics\n",
    "You can now display the training metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# Captured metrics can be accessed as a Pandas dataframe\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And plot the collected metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evals = df[df.metric_name.isin(['eval_accuracy','eval_precision', 'eval_f1'])]\n",
    "losses = df[df.metric_name.isin(['loss', 'eval_loss'])]\n",
    "\n",
    "sns.lineplot(\n",
    "    x='timestamp', \n",
    "    y='value', \n",
    "    data=evals, \n",
    "    style='metric_name',\n",
    "    markers=True,\n",
    "    hue='metric_name'\n",
    ")\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(\n",
    "    x='timestamp', \n",
    "    y='value', \n",
    "    data=losses, \n",
    "    hue='metric_name',\n",
    "    ax=ax2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `transformers pipelines` API allows you to use the `pipelines` features. \n",
    "Your inputs need to be defined in the `inputs` key.\n",
    "If you want additional supported `pipelines` parameters you can add them in the `parameters` key.\n",
    "The API is oriented at the API of the [🤗  Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html), meaning your inputs need to be defined in the `inputs` key and if you want additional supported `pipelines` parameters you can add them in the `parameters` key. \n",
    "\n",
    "Below are a few request examples:\n",
    "\n",
    "**text-classification request body**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**question-answering request body**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"inputs\": {\n",
    "        \"question\": \"What is used for inference?\",\n",
    "        \"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**zero-shot classification request body**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\\\",\n",
    "    \"parameters\": {\n",
    "        \"candidate_labels\": [\n",
    "            \"refund\",\n",
    "            \"legal\",\n",
    "            \"faq\"\n",
    "         ]\n",
    "    }\n",
    "}\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now proceed and create an endpoint with the trained model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the endpoint is deployed you can run a prediction.  `LABEL_0` indicates a negative review, `LABEL_1` indicates a positive review, and score corresponds to the probability of each label."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = {\n",
    "   \"inputs\": [\n",
    "       \"This is a very good product!\",\n",
    "       \"Product is not good at all\",\n",
    "       \"Idea is good, but product quality is poor\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleanup\n",
    "After you are finished experimenting with this notebook, run the following cell to delete the predictor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictor.delete_endpoint()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}